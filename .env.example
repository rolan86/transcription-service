# ==================================================
# Professional Transcription Service Configuration
# ==================================================
# Copy this file to .env and customize for your environment

# Core Transcription Settings
TRANSCRIPTION_MODEL=base
TRANSCRIPTION_LANGUAGE=en
TRANSCRIPTION_CHUNK_DURATION=30
TRANSCRIPTION_FORCE_CHUNKING=false
TRANSCRIPTION_MAX_MEMORY_MB=2000

# Output Configuration
TRANSCRIPTION_OUTPUT_FORMAT=txt
TRANSCRIPTION_INCLUDE_METADATA=true
TRANSCRIPTION_INCLUDE_TIMESTAMPS=false
TRANSCRIPTION_TIMESTAMP_FORMAT=seconds

# Processing Settings
TRANSCRIPTION_TEMP_DIR=
TRANSCRIPTION_CLEANUP_TEMP_FILES=true
TRANSCRIPTION_PROGRESS_REPORTING=true
TRANSCRIPTION_VERBOSE_PROGRESS=false

# Logging Configuration
TRANSCRIPTION_LOG_LEVEL=INFO
TRANSCRIPTION_LOG_FILE=
TRANSCRIPTION_LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# ==================================================
# AI Provider Settings (for cleanup, summarization, analysis)
# ==================================================

# Active AI provider: ollama, claude, zai, or llama
AI_PROVIDER=ollama

# Ollama (Local LLM server - recommended for local use)
# Requires: ollama installed and running (ollama serve)
OLLAMA_MODEL=llama3
OLLAMA_BASE_URL=http://localhost:11434

# Claude (Anthropic API)
# Get your API key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# CLAUDE_MODEL=claude-sonnet-4-20250514

# z.ai (OpenAI-compatible API)
# ZAI_API_KEY=your-zai-key-here
# ZAI_BASE_URL=https://api.z.ai/v1

# Llama.cpp (Local .gguf model file)
# LLAMA_MODEL_PATH=/path/to/your/model.gguf

# ==================================================
# OpenAI Whisper Settings
# ==================================================

# Model Storage and Caching
WHISPER_CACHE_DIR=
WHISPER_DOWNLOAD_ROOT=
WHISPER_DOWNLOAD_TIMEOUT=300
WHISPER_NO_PROGRESS=false

# ==================================================
# Performance and System Settings
# ==================================================

# CPU Threading (adjust based on your CPU cores)
OMP_NUM_THREADS=4
MKL_NUM_THREADS=4

# GPU Settings (uncomment to force CPU usage)
# CUDA_VISIBLE_DEVICES=""

# Memory Management (for GPU users)
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256

# ==================================================
# Network Settings (uncomment if behind proxy)
# ==================================================

# HTTP_PROXY=http://proxy.company.com:8080
# HTTPS_PROXY=http://proxy.company.com:8080
# NO_PROXY=localhost,127.0.0.1,.local

# ==================================================
# Python Settings
# ==================================================

PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1

# ==================================================
# Development/Production Specific
# ==================================================

# Uncomment for development
# TRANSCRIPTION_MODEL=tiny
# TRANSCRIPTION_LOG_LEVEL=DEBUG
# TRANSCRIPTION_VERBOSE_PROGRESS=true

# Uncomment for production
# TRANSCRIPTION_MODEL=large
# TRANSCRIPTION_LOG_LEVEL=INFO
# TRANSCRIPTION_LOG_FILE=/var/log/transcription/app.log